Instructions for running the pipeline on the Compute Canada supercomputers
(clusters) graham, beluga, and cedar.

Ideally requires a Compute Canada account.
If you don't have access to one, then you have two options:

  1) If you want to run the entire pipeline, you will have to run it on your own
  machine, install each of the following programs on your machine, and have the
  executables in your path (versions we used are in in brackets):
  Trimmomatic (0.39), FastQC (0.11.9), SPAdes (3.14.1), bowtie (1.3.0),
  Trinity (2.11.0), jellyfish (2.3.0), salmon (1.3.0), bowtie2 (2.4.1),
  bwa (0.7.17), kraken2 (2.1.1), blast+ (2.11.0), seqtk (1.3), samtools (1.10),
  sortmerna (4.2.0), qt (5.12.8), leveldb (python package, 1.22),
  trans-abyss (2.0.1), MEGAHIT (1.2.9), bedtools (2.29.2),
  justblast (python module, 2020.0.5), ete3 (python module, 3.1.1).

  2) If you just want to run a portion of the pipeline, i.e., just certain program combinations,

Summary:
We have 1536 combinations of programs that we have to run, partially
computationally very expensive. To run these combinations most efficiently,
we run them in parallel by using job arrays that are split over the three
clusters. Therefore, we generate a file that contains one line per combination
of programs (1536 lines) and split this file up into three separate files
(512 lines each). We then use these files to submit job arrays on each cluster,
which means that we submit 512 jobs at the same time where each job reads in a
different line of the program combinations file and executes only these programs.
This allows to run all program combinations in parallel on the three clusters.
We run these for each sample separately and eventually download the final files
on our local computer for further processing.

================================

Preparation:

Databases:
On your local computer, you will have to make a kraken2 and BLAST database
for both NCBI NT and SILVA. This is relatively straight forward for NCBI NT:
download ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nt.gz and follow instructions
here [https://www.ncbi.nlm.nih.gov/books/NBK279688/] to make the NCBI NT BLAST
database (note: make sure to use the -taxid_map option to add taxonomy IDs to
the database) and follow instructions under "Custom databases" for nt here
[https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown] to
make the NCBI NT kraken2 database.
For SILVA, it's less straight forward, therefore, we wrote scripts to take
over this job. They can be found in the directory "subscripts"
(SILVA_SSU_LSU_kraken2_preparation.sh and SILVA_SSU_LSU_makeblastdb_preparation.sh).
The former requires one argument (check the top of the script, usage), the latter
doesn't require any argument and will make the database in the current working directory.

Combination files:
Under the directory "subscripts" you will find the script
generate_pipeline_combinations_file.py. Running this script will generate the three
program combination files that are required to run the pipeline on the clusters,
one combination of programs per line. They will be generated in the current
working directory. Note: we already ran the script and moved the output files
to the directory "pipeline".

Kraken2 files:
Kraken2 on NCBI NT requires some files that are generated via the script
kraken2_required_file_generation.sh under the "subscripts" directory. Run the
script to generate these files in your current working directory.

Programs:
  IDBA:
  We couldn't use the default IDBA program available on Compute Canada but had to edit IDBA prior to compiling it because it didn't work
  using long reads and the -l option. This seems to be a common problem and
  # can be circumvented following for example the instructions in
  # http://seqanswers.com/forums/showthread.php?t=29109, and see also
  # https://github.com/loneknightpy/idba/issues/26. Follow these instruction to compile IDBA yourself.

  barrnap:
  Can be git cloned from GitHub (git clone https://github.com/tseemann/barrnap.git).

  rRNAFilter:
  Is a .jar file and only worked for us when we started it within the directory
  	# containing the .jar file. To simplify switching to that directory, we copy
  	# it from its location to the pwd:



We recommend generating a directory under your Compute Canada account's scratch
directory called "required_files" and moving all generated files into there. The
most convenient way to do so is to generate the files on your own computer and
sending them to the "scratch/required_files" directory on each of the three
clusters using the scp command.

================================

The subscript generate_pipeline_combinations_file.py generates a list with all
pipeline combinations and splits it up into 3 files, one for each compute canada
server (graham, cedar, beluga) with 512 combinations each.

The pipeline now contains several options to replace databases that were hardcoded in the code before.
These are:
  -e  For the directory .etetoolkit/taxa.sqlite. This directory is created in the
      home directory when ete3 is run for the first time. If it's not present in
      home directory, one needs to run ONLY the ete3 command once first, delete
      the output, and then set the -e option to the ~/.etetoolit/taxa.sqlite folder
      that has then been created
  -N  NCBI NT BLAST database
  -S  SILVA BLAST database. To create this database, run the subscript
      SILVA_SSU_LSU_makeblast_preparation.sh. It will generate it automatically.
  -n  NCBI_NT kraken2 database. To make this database, follow the instructions
      here: https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown
      under "Custom databases" and use nt in step 2. One has to run 3 kraken2
      commands. Takes a while.
  -s  SILVA kraken2 database. To create this database, run the subscript SILVA_SSU_LSU_kraken2_preparation.sh.
      It will generate it automatically.
  -B  Bacteria SILVA LSU database for SortMeRNA. Comes with SortMeRNA when the program is installed (silva-bac-23s-id98.fasta)
  -b  Bacteria SILVA SSU database for SortMeRNA. Comes with SortMeRNA when the program is installed (silva-bac-16s-id90.fasta)
  -A  Archaea SILVA LSU database for SortMeRNA. Comes with SortMeRNA when the program is installed (silva-arc-23s-id98.fasta)
  -a  Archaea SILVA SSU database for SortMeRNA. Comes with SortMeRNA when the program is installed (silva-arc-16s-id90.fasta)
  -E  Eukaryota SILVA LSU database for SortMeRNA. Comes with SortMeRNA when the program is installed (silva-euk-28s-id98.fasta)
  -e  Eukaryota SILVA SSU database for SortMeRNA. Comes with SortMeRNA when the program is installed (silva-euk-18s-id95.fasta)
  -R  Rfam 5.8S database for SortMeRNA. Comes with SortMeRNA when the program is installed (rfam-5.8s-database-id98.fasta)
  -r  Rfam 5S database for SortMeRNA. Comes with SortMeRNA when the program is installed (rfam-5s-database-id98.fasta)

Also, the option -T has to be used to refer to the location of the
trimmomatic-<version>.jar program. On top of that, a folder with adapters to
trim must be located in the same folder as the trimmomatic-<version>.jar
application and called "adapters" (that's usually the case when one installs
trimmomatic).

To run every possible combination of tools, the pipeline requires the following
programs/python packages (versions we used when writing this script are
indicated in brackets but can differ from versions loaded on the clusters):
  FastQC (0.11.5), Trimmomatic (0.33), sortmeRNA (4.0.0), barrnap (0.9),
  rRNAFILTER (1.1)[note: is downloaded within the script, doesn't need to be
  installed manually], SPADES (3.14.0)[note: runs with the --meta and --rna
  options for METASPADES and RNASPADES], MEGAHIT (1.2.9), IDBA-UD (1.1.1),
  IDBA-TRAN (1.1.1), Trinity (2.10.0),	bowtie2 (2.3.3.1), bwa (0.7.17),
  blast (2.10.0+), seqtk (1.2-r94),  samtools (1.10),
  python module justblast (2020.0.3), python module ete3 (3.1.2)

Note 1: we had to edit IDBA prior to compiling it because it didn't work
using long reads and the -l option. This seems to be a common problem and
can be circumvented following for example the instructions in
http://seqanswers.com/forums/showthread.php?t=29109, and see also
https://github.com/loneknightpy/idba/issues/26..

EXAMPLE COMMAND ON OUR SERVER:
~/chrisnatjulia/scripts/pipeline/graham/METAGENOMICS_METATRANSCRIPTOMICS_PIPELINE_graham.sh \
-1 ~/pilot_project/reads/M4_DNA/M4_DNA_R2.fastq -2 ~/pilot_project/reads/M4_DNA/M4_DNA_R2.fastq \
-P "5,sortmerna,megahit,bowtie2,silva,kraken2" \
-N /hdd1/databases/nt_database_feb_2020_indexed/nt \
-S /hdd1/databases/SILVA_138.1_SSU_LSURef_NR99_tax_silva_trunc_BLAST_DB_Sep_2020/SILVA_138.1_SSU_LSURef_NR99_tax_silva_trunc.fasta \
-s /hdd1/databases/kraken2_SILVA_138.1_SSU_LSURef_NR99_tax_silva_trunc_DB_Sep_2020 \
-n /hdd1/databases/kraken2_nt_DB -t ~/.etetoolkit/taxa.sqlite \
-a /hdd1/databases/sortmerna_silva_databases/silva-arc-16s-id95.fasta \
-b /hdd1/databases/sortmerna_silva_databases/silva-bac-16s-id90.fasta \
-e /hdd1/databases/sortmerna_silva_databases/silva-euk-18s-id95.fasta \
-E /hdd1/databases/sortmerna_silva_databases/silva-euk-28s-id98.fasta \
-A /hdd1/databases/sortmerna_silva_databases/silva-arc-23s-id98.fasta \
-B /hdd1/databases/sortmerna_silva_databases/silva-bac-23s-id98.fasta \
-R /hdd1/databases/sortmerna_silva_databases/rfam-5.8s-database-id98.fasta \
-r /hdd1/databases/sortmerna_silva_databases/rfam-5s-database-id98.fasta \
-T ~/programs/Trimmomatic-0.39/trimmomatic-0.39.jar -p 32 -m 120G
